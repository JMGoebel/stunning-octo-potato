{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import fetch_arxiv_papers\n",
    "\n",
    "papers = fetch_arxiv_papers('Language Models', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization',\n",
       " 'UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models',\n",
       " 'STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models',\n",
       " 'Performance Evaluation of Large Language Models in Statistical Programming',\n",
       " 'Text2World: Benchmarking Large Language Models for Symbolic World Model Generation',\n",
       " 'SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large Language Models',\n",
       " 'Do we still need Human Annotators? Prompting Large Language Models for Aspect Sentiment Quad Prediction',\n",
       " 'Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents',\n",
       " 'B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability',\n",
       " 'Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[paper[\"title\"] for paper in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "def create_documents_from_papers(papers):\n",
    "    documents = []\n",
    "    for paper in papers:\n",
    "        content = (\n",
    "            f\"Title: {paper['title']}\\n\"\n",
    "            f\"Published: {paper['published']}\\n\"\n",
    "            f\"Authors: {', '.join(paper['authors'])}\\n\"\n",
    "            f\"Journal: {paper['journal_ref']}\\n\"\n",
    "            f\"DOI: {paper['doi']}\\n\"\n",
    "            f\"Primary category: {paper['primary_category']}\\n\"\n",
    "            f\"Categories: {', '.join(paper['categories'])}\\n\"\n",
    "            f\"Summary: {paper['summary']}\\n\"\n",
    "            f\"URL: {paper['pdf_url']}\\n\"\n",
    "            f\"Arxiv URL: {paper['arxiv_url']}\\n\"\n",
    "        )\n",
    "\n",
    "        documents.append(Document(text=content))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = create_documents_from_papers(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='e443a13b-14ff-4dc6-ba87-27ae598b96b7', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization\\nPublished: 2025-02-18 18:59:57+00:00\\nAuthors: Shuo Xing, Yuping Wang, Peiran Li, Ruizheng Bai, Yueqi Wang, Chengxuan Qian, Huaxiu Yao, Zhengzhong Tu\\nJournal: None\\nDOI: None\\nPrimary category: cs.CV\\nCategories: cs.CV, cs.LG\\nSummary: The emergence of large Vision Language Models (VLMs) has broadened the scope\\nand capabilities of single-modal Large Language Models (LLMs) by integrating\\nvisual modalities, thereby unlocking transformative cross-modal applications in\\na variety of real-world scenarios. Despite their impressive performance, VLMs\\nare prone to significant hallucinations, particularly in the form of\\ncross-modal inconsistencies. Building on the success of Reinforcement Learning\\nfrom Human Feedback (RLHF) in aligning LLMs, recent advancements have focused\\non applying direct preference optimization (DPO) on carefully curated datasets\\nto mitigate these issues. Yet, such approaches typically introduce preference\\nsignals in a brute-force manner, neglecting the crucial role of visual\\ninformation in the alignment process. In this paper, we introduce Re-Align, a\\nnovel alignment framework that leverages image retrieval to construct a\\ndual-preference dataset, effectively incorporating both textual and visual\\npreference signals. We further introduce rDPO, an extension of the standard\\ndirect preference optimization that incorporates an additional visual\\npreference objective during fine-tuning. Our experimental results demonstrate\\nthat Re-Align not only mitigates hallucinations more effectively than previous\\nmethods but also yields significant performance gains in general visual\\nquestion-answering (VQA) tasks. Moreover, we show that Re-Align maintains\\nrobustness and scalability across a wide range of VLM sizes and architectures.\\nThis work represents a significant step forward in aligning multimodal LLMs,\\npaving the way for more reliable and effective cross-modal applications. We\\nrelease all the code in https://github.com/taco-group/Re-Align.\\nURL: http://arxiv.org/pdf/2502.13146v1\\nArxiv URL: http://arxiv.org/abs/2502.13146v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='51405fc1-7f1b-4949-ba41-4f21bcc4023f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor Attacks and Adversarial Attacks in Large Language Models\\nPublished: 2025-02-18 18:59:00+00:00\\nAuthors: Huawei Lin, Yingjie Lao, Tong Geng, Tan Yu, Weijie Zhao\\nJournal: None\\nDOI: None\\nPrimary category: cs.CL\\nCategories: cs.CL, cs.AI, cs.LG\\nSummary: Large Language Models (LLMs) are vulnerable to attacks like prompt injection,\\nbackdoor attacks, and adversarial attacks, which manipulate prompts or models\\nto generate harmful outputs. In this paper, departing from traditional deep\\nlearning attack paradigms, we explore their intrinsic relationship and\\ncollectively term them Prompt Trigger Attacks (PTA). This raises a key\\nquestion: Can we determine if a prompt is benign or poisoned? To address this,\\nwe propose UniGuardian, the first unified defense mechanism designed to detect\\nprompt injection, backdoor attacks, and adversarial attacks in LLMs.\\nAdditionally, we introduce a single-forward strategy to optimize the detection\\npipeline, enabling simultaneous attack detection and text generation within a\\nsingle forward pass. Our experiments confirm that UniGuardian accurately and\\nefficiently identifies malicious prompts in LLMs.\\nURL: http://arxiv.org/pdf/2502.13141v1\\nArxiv URL: http://arxiv.org/abs/2502.13141v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1c3e5a6c-7a08-485a-809b-ca2a46232471', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: STEER-ME: Assessing the Microeconomic Reasoning of Large Language Models\\nPublished: 2025-02-18 18:42:09+00:00\\nAuthors: Narun Raman, Taylor Lundy, Thiago Amin, Jesse Perla, Kevin-Leyton Brown\\nJournal: None\\nDOI: None\\nPrimary category: cs.CL\\nCategories: cs.CL\\nSummary: How should one judge whether a given large language model (LLM) can reliably\\nperform economic reasoning? Most existing LLM benchmarks focus on specific\\napplications and fail to present the model with a rich variety of economic\\ntasks. A notable exception is Raman et al. [2024], who offer an approach for\\ncomprehensively benchmarking strategic decision-making; however, this approach\\nfails to address the non-strategic settings prevalent in microeconomics, such\\nas supply-and-demand analysis. We address this gap by taxonomizing\\nmicroeconomic reasoning into $58$ distinct elements, focusing on the logic of\\nsupply and demand, each grounded in up to $10$ distinct domains, $5$\\nperspectives, and $3$ types. The generation of benchmark data across this\\ncombinatorial space is powered by a novel LLM-assisted data generation protocol\\nthat we dub auto-STEER, which generates a set of questions by adapting\\nhandwritten templates to target new domains and perspectives. Because it offers\\nan automated way of generating fresh questions, auto-STEER mitigates the risk\\nthat LLMs will be trained to over-fit evaluation benchmarks; we thus hope that\\nit will serve as a useful tool both for evaluating and fine-tuning models for\\nyears to come. We demonstrate the usefulness of our benchmark via a case study\\non $27$ LLMs, ranging from small open-source models to the current state of the\\nart. We examined each model's ability to solve microeconomic problems across\\nour whole taxonomy and present the results across a range of prompting\\nstrategies and scoring metrics.\\nURL: http://arxiv.org/pdf/2502.13119v1\\nArxiv URL: http://arxiv.org/abs/2502.13119v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3f1f278b-853d-43bc-91ce-4ee159f0cdfd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Performance Evaluation of Large Language Models in Statistical Programming\\nPublished: 2025-02-18 18:37:15+00:00\\nAuthors: Xinyi Song, Kexin Xie, Lina Lee, Ruizhe Chen, Jared M. Clark, Hao He, Haoran He, Jie Min, Xinlei Zhang, Simin Zheng, Zhiyang Zhang, Xinwei Deng, Yili Hong\\nJournal: None\\nDOI: None\\nPrimary category: stat.AP\\nCategories: stat.AP, cs.AI\\nSummary: The programming capabilities of large language models (LLMs) have\\nrevolutionized automatic code generation and opened new avenues for automatic\\nstatistical analysis. However, the validity and quality of these generated\\ncodes need to be systematically evaluated before they can be widely adopted.\\nDespite their growing prominence, a comprehensive evaluation of statistical\\ncode generated by LLMs remains scarce in the literature. In this paper, we\\nassess the performance of LLMs, including two versions of ChatGPT and one\\nversion of Llama, in the domain of SAS programming for statistical analysis.\\nOur study utilizes a set of statistical analysis tasks encompassing diverse\\nstatistical topics and datasets. Each task includes a problem description,\\ndataset information, and human-verified SAS code. We conduct a comprehensive\\nassessment of the quality of SAS code generated by LLMs through human expert\\nevaluation based on correctness, effectiveness, readability, executability, and\\nthe accuracy of output results. The analysis of rating scores reveals that\\nwhile LLMs demonstrate usefulness in generating syntactically correct code,\\nthey struggle with tasks requiring deep domain understanding and may produce\\nredundant or incorrect results. This study offers valuable insights into the\\ncapabilities and limitations of LLMs in statistical programming, providing\\nguidance for future advancements in AI-assisted coding systems for statistical\\nanalysis.\\nURL: http://arxiv.org/pdf/2502.13117v1\\nArxiv URL: http://arxiv.org/abs/2502.13117v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3d59dcb2-5b49-48f9-9531-5789cf07819d', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Text2World: Benchmarking Large Language Models for Symbolic World Model Generation\\nPublished: 2025-02-18 17:59:48+00:00\\nAuthors: Mengkang Hu, Tianxing Chen, Yude Zou, Yuheng Lei, Qiguang Chen, Ming Li, Hongyuan Zhang, Wenqi Shao, Ping Luo\\nJournal: None\\nDOI: None\\nPrimary category: cs.CL\\nCategories: cs.CL, cs.AI\\nSummary: Recently, there has been growing interest in leveraging large language models\\n(LLMs) to generate symbolic world models from textual descriptions. Although\\nLLMs have been extensively explored in the context of world modeling, prior\\nstudies encountered several challenges, including evaluation randomness,\\ndependence on indirect metrics, and a limited domain scope. To address these\\nlimitations, we introduce a novel benchmark, Text2World, based on planning\\ndomain definition language (PDDL), featuring hundreds of diverse domains and\\nemploying multi-criteria, execution-based metrics for a more robust evaluation.\\nWe benchmark current LLMs using Text2World and find that reasoning models\\ntrained with large-scale reinforcement learning outperform others. However,\\neven the best-performing model still demonstrates limited capabilities in world\\nmodeling. Building on these insights, we examine several promising strategies\\nto enhance the world modeling capabilities of LLMs, including test-time\\nscaling, agent training, and more. We hope that Text2World can serve as a\\ncrucial resource, laying the groundwork for future research in leveraging LLMs\\nas world models. The project page is available at\\nhttps://text-to-world.github.io/.\\nURL: http://arxiv.org/pdf/2502.13092v1\\nArxiv URL: http://arxiv.org/abs/2502.13092v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0153c815-e201-485c-b2a7-1779349ed33a', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large Language Models\\nPublished: 2025-02-18 17:04:26+00:00\\nAuthors: Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, Yutao Zeng, Zhoufutu Wen, Ke Jin, Baorui Wang, Weixiao Zhou, Yunhong Lu, Tongliang Li, Wenhao Huang, Zhoujun Li\\nJournal: None\\nDOI: None\\nPrimary category: cs.CL\\nCategories: cs.CL\\nSummary: The increasing application of multi-modal large language models (MLLMs)\\nacross various sectors have spotlighted the essence of their output reliability\\nand accuracy, particularly their ability to produce content grounded in factual\\ninformation (e.g. common and domain-specific knowledge). In this work, we\\nintroduce SimpleVQA, the first comprehensive multi-modal benchmark to evaluate\\nthe factuality ability of MLLMs to answer natural language short questions.\\nSimpleVQA is characterized by six key features: it covers multiple tasks and\\nmultiple scenarios, ensures high quality and challenging queries, maintains\\nstatic and timeless reference answers, and is straightforward to evaluate. Our\\napproach involves categorizing visual question-answering items into 9 different\\ntasks around objective events or common knowledge and situating these within 9\\ntopics. Rigorous quality control processes are implemented to guarantee\\nhigh-quality, concise, and clear answers, facilitating evaluation with minimal\\nvariance via an LLM-as-a-judge scoring system. Using SimpleVQA, we perform a\\ncomprehensive assessment of leading 18 MLLMs and 8 text-only LLMs, delving into\\ntheir image comprehension and text generation abilities by identifying and\\nanalyzing error cases.\\nURL: http://arxiv.org/pdf/2502.13059v1\\nArxiv URL: http://arxiv.org/abs/2502.13059v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='d82de2ba-b95c-4f60-8f7f-1c47b061510e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Do we still need Human Annotators? Prompting Large Language Models for Aspect Sentiment Quad Prediction\\nPublished: 2025-02-18 16:56:15+00:00\\nAuthors: Nils Constantin Hellwig, Jakob Fehle, Udo Kruschwitz, Christian Wolff\\nJournal: None\\nDOI: None\\nPrimary category: cs.CL\\nCategories: cs.CL\\nSummary: Aspect sentiment quadruple prediction (ASQP) facilitates a detailed\\nunderstanding of opinions expressed in a text by identifying the opinion term,\\naspect term, aspect category and sentiment polarity for each opinion. However,\\nannotating a full set of training examples to fine-tune models for ASQP is a\\nresource-intensive process. In this study, we explore the capabilities of large\\nlanguage models (LLMs) for zero- and few-shot learning on the ASQP task across\\nfive diverse datasets. We report F1 scores slightly below those obtained with\\nstate-of-the-art fine-tuned models but exceeding previously reported zero- and\\nfew-shot performance. In the 40-shot setting on the Rest16 restaurant domain\\ndataset, LLMs achieved an F1 score of 52.46, compared to 60.39 by the\\nbest-performing fine-tuned method MVP. Additionally, we report the performance\\nof LLMs in target aspect sentiment detection (TASD), where the F1 scores were\\nalso close to fine-tuned models, achieving 66.03 on Rest16 in the 40-shot\\nsetting, compared to 72.76 with MVP. While human annotators remain essential\\nfor achieving optimal performance, LLMs can reduce the need for extensive\\nmanual annotation in ASQP tasks.\\nURL: http://arxiv.org/pdf/2502.13044v1\\nArxiv URL: http://arxiv.org/abs/2502.13044v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='281b16f1-a804-490d-907e-617c84cde07f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents\\nPublished: 2025-02-18 16:33:33+00:00\\nAuthors: Chaoran Chen, Bingsheng Yao, Ruishi Zou, Wenyue Hua, Weimin Lyu, Toby Jia-Jun Li, Dakuo Wang\\nJournal: None\\nDOI: None\\nPrimary category: cs.HC\\nCategories: cs.HC, cs.CL\\nSummary: Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that\\nsimulates human-like behaviors in a variety of tasks. However, evaluating RPAs\\nis challenging due to diverse task requirements and agent designs. This paper\\nproposes an evidence-based, actionable, and generalizable evaluation design\\nguideline for LLM-based RPA by systematically reviewing 1,676 papers published\\nbetween Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes,\\nseven task attributes, and seven evaluation metrics from existing literature.\\nBased on these findings, we present an RPA evaluation design guideline to help\\nresearchers develop more systematic and consistent evaluation methods.\\nURL: http://arxiv.org/pdf/2502.13012v1\\nArxiv URL: http://arxiv.org/abs/2502.13012v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3ae8eb93-da7c-42f4-a7d9-16a055a481dd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability\\nPublished: 2025-02-18 16:13:08+00:00\\nAuthors: Yifan Wang, Sukrut Rao, Ji-Ung Lee, Mayank Jobanputra, Vera Demberg\\nJournal: None\\nDOI: None\\nPrimary category: cs.CL\\nCategories: cs.CL, cs.AI\\nSummary: Post-hoc explanation methods for black-box models often struggle with\\nfaithfulness and human interpretability due to the lack of explainability in\\ncurrent neural models. Meanwhile, B-cos networks have been introduced to\\nimprove model explainability through architectural and computational\\nadaptations, but their application has so far been limited to computer vision\\nmodels and their associated training pipelines. In this work, we introduce\\nB-cos LMs, i.e., B-cos networks empowered for NLP tasks. Our approach directly\\ntransforms pre-trained language models into B-cos LMs by combining B-cos\\nconversion and task fine-tuning, improving efficiency compared to previous\\nB-cos methods. Our automatic and human evaluation results demonstrate that\\nB-cos LMs produce more faithful and human interpretable explanations than post\\nhoc methods, while maintaining task performance comparable to conventional\\nfine-tuning. Our in-depth analysis explores how B-cos LMs differ from\\nconventionally fine-tuned models in their learning processes and explanation\\npatterns. Finally, we provide practical guidelines for effectively building\\nB-cos LMs based on our findings. Our code is available at\\nhttps://anonymous.4open.science/r/bcos_lm.\\nURL: http://arxiv.org/pdf/2502.12992v1\\nArxiv URL: http://arxiv.org/abs/2502.12992v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='7d4050eb-208e-45cb-9c15-fa1c6abc32f6', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"Title: Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking\\nPublished: 2025-02-18 15:48:46+00:00\\nAuthors: Junda Zhu, Lingyong Yan, Shuaiqiang Wang, Dawei Yin, Lei Sha\\nJournal: None\\nDOI: None\\nPrimary category: cs.CL\\nCategories: cs.CL\\nSummary: The reasoning abilities of Large Language Models (LLMs) have demonstrated\\nremarkable advancement and exceptional performance across diverse domains.\\nHowever, leveraging these reasoning capabilities to enhance LLM safety against\\nadversarial attacks and jailbreak queries remains largely unexplored. To bridge\\nthis gap, we propose Reasoning-to-Defend (R2D), a novel training paradigm that\\nintegrates safety reflections of queries and responses into LLMs' generation\\nprocess, unlocking a safety-aware reasoning mechanism. This approach enables\\nself-evaluation at each reasoning step to create safety pivot tokens as\\nindicators of the response's safety status. Furthermore, in order to improve\\nthe learning efficiency of pivot token prediction, we propose Contrastive Pivot\\nOptimization(CPO), which enhances the model's ability to perceive the safety\\nstatus of dialogues. Through this mechanism, LLMs dynamically adjust their\\nresponse strategies during reasoning, significantly enhancing their defense\\ncapabilities against jailbreak attacks. Extensive experimental results\\ndemonstrate that R2D effectively mitigates various attacks and improves overall\\nsafety, highlighting the substantial potential of safety-aware reasoning in\\nstrengthening LLMs' robustness against jailbreaks.\\nURL: http://arxiv.org/pdf/2502.12970v1\\nArxiv URL: http://arxiv.org/abs/2502.12970v1\\n\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from constants import embed_model\n",
    "\n",
    "Settings.chunk_size = 1024  \n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist('index/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
